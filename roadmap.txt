A simple text file outlining future features and how to get there
=================================================================

FEATURE PACKAGES
----------------
Full streaming through Cedar client and server
+ Start/Stop recording
+ Use webopus to encode/decode in separate threads
+ Disable input selection while recording
+ Musicians placed in chain automatically
x Only root musician can start recording (eventually host maybe?)
+ Audio recording delayed by x seconds per musician
+ Make sure we don't needlessly create audio nodes when switching audio inputs
+ Features
  + New MasterOutput component that launches the RoomAudioPlayer node and
    includes a gain control and volume bar
  + AudioInputBufferer, which takes raw audio input PCM data, buffers it for a
    bit, then sends it back out to the main thread
  + VolumeBar now displays audio decibel-scaled from 0 to -40 dB
+ Don't patch so crazy man. DONE

Put it on DigitalOcean
+ Put it on a droplet
- Build the electron app for MacOS and Windows
- Test that it works on Windows
- See if it works with friends!
- Set up environment solution

Loopback delay calculation
- Play a ping of a certain frequency
- Use a bandpass filter to find the peak
- Measure the delay
- (bonus) Visualize the audio received by the client!
- This value should be manually editable as well

Modifying the chain
- Two lists: audience and musicians; can move participants between lists
- Musicians are not added to the chain automatically
- We can set the delay between musicians manually
- Need to be able to set musician names
  - Simple: shows name in top-right corner--can edit the field to update name
  - Defaults to a bird or type of tree (probably a bird)
  - Name saved somehow?
  - In the future, we'll let people have profiles with names, and people can
    change their name for the room like in Zoom
- Cannot update chain during recording
- Last musician in chain is considered the mixer

Client-side mixing
- Optimize volume bar. Is rendering slow? Is getting data slow?
- Can only see musicians behind you
- Tooltip about the fact that this only mixes for you (maybe just call it
  "Personal Mixer"?)
- Master output and slider

Set up authentication
- Google auth?

Downtimeless updates
- Use infrastructure-as-code to provision Cedar server
- Use load balancer for downtimeless deploys
- Automatically upload Electron app to CMS
- Landing page?
- Accounts with subscriptions?

Other bugs/features before wider release
- Microphone permission detection broken in Electron
- Auto-updates https://www.electron.build/auto-update
- Auth

Wider release milestone!

Make web app version public (for ChromeOS, etc.)
- autoplay-policy makes me sad, and we need to get around it: Call
  audiocontext.resume on user interaction? (not necessary in electron)
  https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio

Allow rejoining stream mid-stream (if you lose your place entirely). This can
be for audience members first, though we should be able to do it for musicians
as well, in theory.
Currently, when a recording starts, we assume:
1. that all clients will start recording at approximately the correct time
2. that all clients receive perfect data from the server (no gaps)
3. that all clients perfectly send *all* input device data to the server (no
   gaps)
I am reasonably confident that we can rely on assumptions (1) and (2) holding
true until we see evidence otherwise. Though I am not as confident about (3)
because I don't know whether the Web Audio API guarantees all audio data passes
through the audio graph lossless (though I would be surprised if that weren't
the case), and the bigger factor: computers freeze and lose connection. In
these cases, we will want a way for clients to jump back into the action, even
without knowing the exact time recording began. To achieve this, I believe we
need these things:
* You can fetch the last x seconds (probably however long the chain is) of
  audio data from the server
* Each chunk of audio data includes an index that corresponds to that index of
  the root audio track
* Each client attaches that index to each chunk they send out
* Clients only buffer audio for playing that matches the position relative to
  the root audio (e.g., there can be gaps in data, but we should be able to
  identify them)
Then and only then can a client hypothetically join an existing stream,
starting wherever they need to relative to root audio (given their position in
the chain).
- Need to save metadata in stream chunks--i.e., chunk index

Cedar audience
- Can join in whenever and hear the audio of the room (definitely need
  rejoining mid-stream to be a thing for this)

FEATURE PACKAGES THAT NEED TO BE SPECCED OUT
--------------------------------------------
Future features
- Use JWT + Google OAuth for authentication
- Display errors (esp. for streaming failures)
- Server track PATCH endpoint is truly idempotent (returns the same response if
  the same input is received). This will help make us robust to servers that
  die (i.e., it's OK if we don't get the new cursor immediately when writing
  audio data)
- Support multiple channels (stereo+)
- Visualize and allow playback of recordings
- Download recordings
- Lossless audio
- Musician layers/lanes for grouping musicians together
  - Includes server-side mixing of each layer to minimize network data flux
- Possibly put non-audio-streaming data into an RDBMS instead of Redis
- Intercom feature? I really, really hope we don't need this (managing multiple
  input devices sounds like UX hell). But it may be better than using two
  applications for some people

WHEN I GET BORED OR IF SOMEONE JOINS THE PROJECT
------------------------------------------------
- Documentation on how the frontend sends and receives audio data (include how
  we compiled webopus)

MISC NOTES
----------
OK, so stuff about audio glitches
- Link to resources on debugging
- We need to do time stretching for microphone skips (happens sometimes, and we
  CANNOT tolerate it because we need sample n on every audio stream to refer to
  the same time point). We currently play the same frame twice.
- Sometimes rendering audio takes a long time, and we end up with underflow on
  the speaker end of things. We can *maybe* help with this by using a
  SharedArrayBuffer to cut down on message-posting costs. Other than that, I'm
  not sure what's best here.

Outgoing worklet:
- audio input piped into this
- load wasm script on instantiation
- once started, buffer raw PCM data from input, then encode a frame (0.2
  seconds?) and post the message to the main thread
- main thread sends this out to Cedar server
- should update the patch endpoint to, if it receives data with the wrong
  cursor,
  * if the incoming data matches the data from that cursor onward, return the
    saved cursor with a 200 (truly idempotent, which is handy for when a server
    dies before responding)
  * if the incoming data does not match the data from that cursor onward, fail
    the request by saying something like, "The provided data is not contiguous
    with/adjacent to previous data"

Incoming worklet:
- piped into the audio context destination
- load wasm script on instantiation
- main thread fetches audio data and sends opus-encoded sets of frames from all
  incoming streams to the worklet.
- After {musicianIndex * chainDelay} time, "start" will be sent to this worklet
  and to "Outgoing worklet" at the same time to sync our musician's data with
  the root musician's data.
- received data sets will be decoded and added to a buffer (decoded in this
  async callback to keep the main process() loop fast)
- when started, the data from the buffer will be progressively loaded into the
  worklet's output
- make sure to free up the buffer periodically so we don't keep raw PCM data
  sitting in memory forever
- NOTE: When we allow client-side gain alteration, this should be handled as a
  worklet param so we can update gain dynamically in real time during playback.
