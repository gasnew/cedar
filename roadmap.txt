A simple text file outlining future features and how to get there
=================================================================

FEATURE PACKAGES
----------------
Full streaming through Cedar client and server
+ Start/Stop recording
+ Use webopus to encode/decode in separate threads
+ Disable input selection while recording
+ Musicians placed in chain automatically
x Only root musician can start recording (eventually host maybe?)
+ Audio recording delayed by x seconds per musician
+ Make sure we don't needlessly create audio nodes when switching audio inputs
+ Features
  + New MasterOutput component that launches the RoomAudioPlayer node and
    includes a gain control and volume bar
  + AudioInputBufferer, which takes raw audio input PCM data, buffers it for a
    bit, then sends it back out to the main thread
  + VolumeBar now displays audio decibel-scaled from 0 to -40 dB
+ Don't patch so crazy man. DONE

Put it on DigitalOcean
+ Put it on a droplet
+ Build the electron app for MacOS and Windows
+ Test that it works on Windows
x See if it works with friends!
+ Set up environment solution

Modifying the chain
+ Two lists: audience and musicians; can move participants between lists
+ Musicians are not added to the chain automatically
+ We can set the delay between musicians manually
+ Need to be able to set musician names
  + Simple: shows name in top-right corner--can edit the field to update name
  + Defaults to a bird or type of tree (probably a bird)
  + Name saved somehow?
  + In the future, we'll let people have profiles with names, and people can
    change their name for the room like in Zoom
+ Cannot update chain during recording
+ Last musician in chain is considered the mixer; first considered host?
+ Real-time name updates
+ Optimize volume bar. Rendering with react-konva was costly
+ Don't let musicians be multiline

Loopback delay calculation
+ Play a ping of a certain frequency (use a "chirp" signal?)
+ Use a bandpass filter to find the peak
+ Measure the delay
x (bonus) Visualize the audio received by the client!
+ This value should be manually editable as well
+ Loopback latency calibration closes if recording starts, and button is
  disabled during recording
+ Default the calibration input to the canonical input
+ Include a volume bar in the calibration card
+ Default loopback latency to nothing, and require setting it before being
  added to the chain
+ Mix multiple input channels into one

Client-side mixing
+ Can only see musicians behind you
+ Tooltip about the fact that this only mixes for you (maybe just call it
  "Personal Mixer"?)
+ Master output and slider
+ Cut down on needless redux actions (from onUpdate calls?)
+ Support musicians disconnecting
+ Seconds between musicians has a minimum of ~0.4
+ Fix dB-setting/resetting
+ "Alpha" indicator
+ Fix link
+ Update README

Penultimate Patreon push
+ Fix playback buffering issues (not getting zeroed out)
+ Start recording + playback at exactly the same time (probably by adjusting to
  recordingStartedAt)
+ Start recording based on ws and/or polling. Stopping is still just polling
+ More forgiving input and output "catch up" thresholds
+ Need to use both channels with loopback latency calibration
+ Merge listening channels into one
+ Rename "Listen" to something clearer--"Monitor," "Listen to this," "Hear myself," "Test my sound"?

Final push prior to Patreon
+ Signed and notarized distributions
+ Auto-updates https://www.electron.build/auto-update
+ http://cedarapp.org
+ auto-restart prod server
+ fix recording state getting broken
+ show version number

Improve Cedar interoperability experience
+ Allow setting output audio device
+ Disable "Monitor input" while recording

Admin stuff (no code)
+ Write how-to guide
- Demo video (~5 minutes, including musical performance)

Fix bugs
- delete recordings when done
- Somehow, playback buffers don't always get zero'd out (still!)
- Set up loopback calibration for non-mic instruments
- Do testing with audio interface?
- Possibly move to MediaRecorder
  (https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaStreamDestination)
  and AudioContext.decodeAudioData
  (https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/decodeAudioData)
  for encoding/decoding rather than webopus?
  - I'm concerned that webopus is responsible for out-of-order delivery and
    possibly other audio glitches...
- Input selected (and seen in volume bar) may be different than the one sending
  audio through the chain...? Haven't seen this in a while
- Dynamically detect audio quantum size...?
- Sometimes output is crunchy (maybe due to playback catch-up method...)
- Sometimes output is choppy
- Output becomes choppy after long recording time?
- Sometimes screen turns white
- Think about including the sample index of the root track that all other data
  packets are relative to instead of adjusting based on time (should prevent
  stream from degrading over time)

Quality-of-life improvements
- Musician stream health indicators
  - Each musician client reports how much buffer room it has
  - A buffer is healthier the closer that value is to secondsBetweenMusicians
  - This is in the UI somehow as a generic "bad", "okay", "good" rating or the
    number itself
  - This would be extremely useful for debugging and could indicate to a group
    if they could reduce secondsBetweenMusicians safely
- Allow copy/paste via right-click (custom electron context menu)
- Fix audio choppyness
- Fix loopback calibration not working for everyone (maybe fixed)
- Indication of when WebSocket connection is lost
- Button to "fix audio problems" (short-term solution to resolving audio
  cutting in and out)
- Can live-connect/disconnect audio input devices
- Animated brightness to show audio progression through chain
- Make name obviously editable
- Select your instrument (need to find instrument pack)
- The host is the one who creates the room?
- Button musicians can click that indicate something is wrong
x Standby scrollable?
+ "Listen to mic" button
+ Rename "Audience" to "Standby"
+ Don't play room audio when on Standby
+ Remove musicians when they leave the room
+ Persist redux state on a per-tab basis (nice during development)
- Be able to leave the room and join another (need to remember to update
  channel subscriptions)

Saving data
- Remember name
- Remember loopback latency by device

Download recordings
- Mixer role?
- Downloads final mix and individual files as zip
- Use bull (https://github.com/OptimalBits/bull/) for async jobs?
  (https://devcenter.heroku.com/articles/node-redis-workers)
- Can pick where to save files--defaults somewhere

Mixer role
- Extra slot below the chain
- Sends their mix directly to the server (what happens if nobody takes this
  role, but we still want to download the mix?)
- This is what the audience hears
- Standby musicians don't have to be able to hear if they join late (for now)
- This is included in the recording download
- Can edit clients' loopback latency (little gear popover?)

Audience app
- Unique link for every room--"click to copy audience invitation link"
- Separate React(?) app
- Indicates whether music is playing
- Plays the stream from the mixer

In-app chat
- Actually use websocket subscriptions?
- People can send links

Set up authentication
- Google auth?

Downtimeless updates
- Use infrastructure-as-code to provision Cedar server
- Use load balancer for downtimeless deploys
- Automatically upload Electron app to CMS
- Landing page?
- Accounts with subscriptions?

Other bugs/features before wider release
- Landing page
- Sign up/in page
- Subscription method??
- Remembering being logged in
- Display errors (esp. for streaming failures)
- Calculate seconds between musicians using channels
- Free trial?
  - Possibly decrease audio quality after 15 minutes, limit to 4 musicians,
    limit to 2-minute recordings (with toast notification), etc.
- Microphone permission detection broken in Electron
- Auth
+ enter and exit room ("active" and "inactive" users... Inactive users in the
  chain are removed from the chain when recording stops)
- Use react-beautiful-dnd sensors to make other people's list changes look less
  janky?
- Little musical instrument icons?
- Use an LB with multiple droplets (or use container orchestration) rather than
  installing everything (including SSL) on a droplet by hand

Wider release milestone!

Make web app version public (for ChromeOS, etc.)
- autoplay-policy makes me sad, and we need to get around it: Call
  audiocontext.resume on user interaction? (not necessary in electron)
  https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio

Allow rejoining stream mid-stream (if you lose your place entirely). This can
be for audience members first, though we should be able to do it for musicians
as well, in theory.
Currently, when a recording starts, we assume:
1. that all clients will start recording at approximately the correct time
2. that all clients receive perfect data from the server (no gaps)
3. that all clients perfectly send *all* input device data to the server (no
   gaps)
I am reasonably confident that we can rely on assumptions (1) and (2) holding
true until we see evidence otherwise. Though I am not as confident about (3)
because I don't know whether the Web Audio API guarantees all audio data passes
through the audio graph lossless (though I would be surprised if that weren't
the case), and the bigger factor: computers freeze and lose connection. In
these cases, we will want a way for clients to jump back into the action, even
without knowing the exact time recording began. To achieve this, I believe we
need these things:
* You can fetch the last x seconds (probably however long the chain is) of
  audio data from the server
* Each chunk of audio data includes an index that corresponds to that index of
  the root audio track
* Each client attaches that index to each chunk they send out
* Clients only buffer audio for playing that matches the position relative to
  the root audio (e.g., there can be gaps in data, but we should be able to
  identify them)
Then and only then can a client hypothetically join an existing stream,
starting wherever they need to relative to root audio (given their position in
the chain).
- Need to save metadata in stream chunks--i.e., chunk index

Cedar audience
- Can join in whenever and hear the audio of the room (definitely need
  rejoining mid-stream to be a thing for this)

FEATURE PACKAGES THAT NEED TO BE SPECCED OUT
--------------------------------------------
Future features
- Use JWT + Google OAuth for authentication
- Display errors (esp. for streaming failures)
  - Maybe allow handle in FeathersHooks to make these pop up if I don't handle
    them in a particular way?
- Server track PATCH endpoint is truly idempotent (returns the same response if
  the same input is received). This will help make us robust to servers that
  die (i.e., it's OK if we don't get the new cursor immediately when writing
  audio data)
- Support multiple channels (stereo+)
- Visualize and allow playback of recordings
- Download recordings
- Lossless audio
- Musician layers/lanes for grouping musicians together
  - Includes server-side mixing of each layer to minimize network data flux
- Possibly put non-audio-streaming data into an RDBMS instead of Redis
- Play track (file, built-in metronome, etc.) alongside root musician (can be
  mixed separately)
- Intercom feature? I really, really hope we don't need this (managing multiple
  input devices sounds like UX hell). But it may be better than using two
  applications for some people
  - One advantage is that it would prevent us from needing to toggle mute on
    and off in Zoom; also doesn't require people to have a separate
    conferencing service (manage two links); UX greatly improved...?
- how to select master output? <-- This was confusion about where the output
  audio goes. I think the OS controls this?
  - Here's a thread about it:
    https://stackoverflow.com/questions/41863094/how-to-select-destination-output-device-using-web-audio-api

WHEN I GET BORED OR IF SOMEONE JOINS THE PROJECT
------------------------------------------------
- Documentation on how the frontend sends and receives audio data (include how
  we compiled webopus)

MISC NOTES
----------
OK, so stuff about audio glitches
+ Link to resources on debugging
- We need to do time stretching for microphone skips (happens sometimes, and we
  CANNOT tolerate it because we need sample n on every audio stream to refer to
  the same time point). We currently play the same frame twice.
- Sometimes rendering audio takes a long time, and we end up with underflow on
  the speaker end of things. We can *maybe* help with this by using a
  SharedArrayBuffer to cut down on message-posting costs. Other than that, I'm
  not sure what's best here.
= Send sine wave through Cedar, and inspect resultant waveform
  - I suspect this is due to how we recover from Web Audio API overbuffering
    (queue size > 1) or somehow skipping frames in output (less likely)
  - It is in part the above I think. One thing to probably rule out is opus
    encoding--it does appear that we're encoding/decoding all the data, as
    evidenced by the fact that tracks stay in-sync long-term (our syncing logic
    makes sure we're churning through samples 1:1, but if data were lost,
    tracks would get out of sync)
  - This issue still occurs, even if I disable our syncing logic

Outgoing worklet:
- should update the patch endpoint to, if it receives data with the wrong
  cursor,
  * if the incoming data matches the data from that cursor onward, return the
    saved cursor with a 200 (truly idempotent, which is handy for when a server
    dies before responding)
  * if the incoming data does not match the data from that cursor onward, fail
    the request by saying something like, "The provided data is not contiguous
    with/adjacent to previous data"

Incoming worklet:
- NOTE: When we allow client-side gain alteration, this should be handled as a
  worklet param so we can update gain dynamically in real time during playback.

CEDAR TESTING SESSIONS
----------------------
9/28/20
+ how to select master output? <-- This was confusion about where the output
  audio goes. I think the OS controls this?
+ playback button (listen to mic)
+ loopback latency is a pain to set
+ didn’t know could change name—why am I this weird name?
+ worthwhile to see others’ latency? compared to you or to the original
    + know how long to wait for all musicians to hear audio
    + IDEA TIME: Oscillating brightness (like vegas lights) to see sound
      progress through chain
+ in-app chat/way to type messages
+ way to label which instrument (use emoji?)
+ in-app intercom—automatically toggle mute
+ nice to be able to adjust input volume
+ official host designation
+ arbitrary 8-person audience limit
+ remove musicians when leave
+ standby lobby mode—talk and interact (calob)
    + could even be nice for rebel worship scenarios
    + downside could be everyone downloads it
+ mixer role (adjust other people’s latencies & sound) could be admin or secondary role
x public room list—privacy rooms via passwords
x browse list of rooms + number of members
+ audience-only version? webpage? volume controls?
+ share lyrics/pdfs/chord chart
+ native recording
+ app is good! webapp for audience-only version maybe
+ way to mock out other musicians (play in base track). way to experience by oneself. good for click track or base track
+ light mode is ugly
+ note from me to me: be clearer about agenda ahead of time
+ garrett loopback: 90
+ calob loopback: 90 (unverified)
+ daniel loopback: 70
+ emily loopback: 170
+ aly loopback: 175
+ isaac loopback: 145
+ “click to copy audience invitation link”
+ chirp for loopback?
+ looks clean as heck —miranda

10/17/20
x Replay recording and mix live--could also determine loopback latency from this?
  - Deferred. Loopback latency will be able to be adjusted live by the mixer.
    Unclear what the value of this would be otherwise.
+ Measuring loopback latency takes ~5 minutes per musician
+ Sound quality was actually really good
+ White screen of death occurred occasionally (should be easier to solve after
  we capture and display errors properly)

11/13/20
+ Need to use both channels with loopback latency calibration
+ Merge listening channels into one
+ Look into pasting
+ Save loopback latency
+ Paul heard bits of guitar during a capella part
+ Check code for clearing room audio ring buffer (clips of Dave on guitar)
+ Synchronize input and output once and for all (slower CPUs maybe have greater loopback latency)
+ Rename "Listen" to something clearer--"Monitor," "Listen to this," "Hear myself," "Test my sound"?
+ Think about monitoring from audio interface--seems to introduce 10 ms of latency for me (probably not the cause of Paul + Jenn's issues)
+ Feature bounty (one big, one small every month)
+ Still want to do Patroen ASAP
+ Separate running open source project from supporting companies (being on-call, custom installation, etc.)
+ Root musician and upcoming Mixer alone can start and stop recordings--all musicians can indicate "something is going wrong," which will cause their nametag to flash red
+ Answer immediate question; then answer philosophical aspect if necessary
+ Mixing channels separately would be toit
+ "Medicine for a weary soul"
+ Calculate seconds between musicians using channels
