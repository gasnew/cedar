A simple text file outlining future features and how to get there
-----------------------------------------------------------------

TODO: Put this data into actual issue tracking software

Audio streaming
I'm hoping to use Web Assembly for encoding/decoding because it's like 10x
faster than plain JS. But for this, we need to load/compile a .wasm module in
an audio worklet or web worker (EDIT: probably a web worker; will look into
this), which can be a sync or async operation. We *may* need, then, to post a
message from the worklet when this is complete, but I'm not 100% sure we need
to since we should be instantiating the worklet as soon as an input audio
device is selected (should be sufficiently long before initiating recording).

Then, to start recording, the main thread will post a message to the audio
worklet for outgoing audio and to the audio worklet for incoming audio to say
"start"

Outgoing worklet:
- audio input piped into this
- load wasm script on instantiation
- once started, buffer raw PCM data from input, then encode a frame (0.2
  seconds?) and post the message to the main thread
- main thread sends this out to Cedar server
- should update the patch endpoint to, if it receives data with the wrong
  cursor,
  * if the incoming data matches the data from that cursor onward, return the
    saved cursor with a 200 (truly idempotent, which is handy for when a server
    dies before responding)
  * if the incoming data does not match the data from that cursor onward, fail
    the request by saying something like, "The provided data is not contiguous
    with/adjacent to previous data"

Incoming worklet:
- piped into the audio context destination
- load wasm script on instantiation
- main thread fetches audio data and sends opus-encoded sets of frames from all
  incoming streams to the worklet.
- After {musicianIndex * chainDelay} time, "start" will be sent to this worklet
  and to "Outgoing worklet" at the same time to sync our musician's data with
  the root musician's data.
- received data sets will be decoded and added to a buffer (decoded in this
  async callback to keep the main process() loop fast)
- when started, the data from the buffer will be progressively loaded into the
  worklet's output
- make sure to free up the buffer periodically so we don't keep raw PCM data
  sitting in memory forever
- NOTE: When we allow client-side gain alteration, this should be handled as a
  worklet param so we can update gain dynamically in real time during playback.

- Start/Stop recording
- Musicians placed in chain automatically
- Only root musician can start recording (eventually host maybe?)
- Audio recording delayed by 1 second per musician

Put it on DigitalOcean
- Put it on a droplet
- See if it works with friends!

Modifying the chain
- Two lists: audience and musicians; can move participants between lists
- Musicians are not added to the chain automatically
- Need to be able to set musician names
  - Simple: shows name in top-right corner--can edit the field to update name
  - Defaults to a bird or type of tree (probably a bird)
  - Name saved somehow?
  - In the future, we'll let people have profiles with names, and people can
    change their name for the room like in Zoom
- Cannot update chain during recording
- Last musician in chain is considered the mixer

Client-side mixing
- Optimize volume bar. Is rendering slow? Is getting data slow?
- Can only see musicians behind you
- Tooltip about the fact that this only mixes for you (maybe just call it
  "Personal Mixer"?)
- Master output and slider

Allow rejoining stream mid-stream (if you lose your place entirely). This can
be for audience members first, though we should be able to do it for musicians
as well, in theory.
Currently, when a recording starts, we assume:
1. that all clients will start recording at approximately the correct time
2. that all clients receive perfect data from the server (no gaps)
3. that all clients perfectly send *all* input device data to the server (no
   gaps)
I am reasonably confident that we can rely on assumptions (1) and (2) holding
true until we see evidence otherwise. Though I am not as confident about (3)
because I don't know whether the Web Audio API guarantees all audio data passes
through the audio graph lossless (though I would be surprised if that weren't
the case), and the bigger factor: computers freeze and lose connection. In
these cases, we will want a way for clients to jump back into the action, even
without knowing the exact time recording began. To achieve this, I believe we
need these things:
* You can fetch the last x seconds (probably however long the chain is) of
  audio data from the server
* Each chunk of audio data includes an index that corresponds to that index of
  the root audio track
* Each client attaches that index to each chunk they send out
* Clients only buffer audio for playing that matches the position relative to
  the root audio (e.g., there can be gaps in data, but we should be able to
  identify them)
Then and only then can a client hypothetically join an existing stream,
starting wherever they need to relative to root audio (given their position in
the chain).
- Need to save metadata in stream chunks--i.e., chunk index

Cedar audience
- Can join in whenever and hear the audio of the room (definitely need
  rejoining mid-stream to be a thing for this)

Future features
- Support multiple channels (stereo+)
- Visualize and allow playback of recordings
- Download recordings
- Lossless audio
- Musician layers/lanes for grouping musicians together
  - Includes server-side mixing of each layer to minimize network data flux
- Possibly put non-audio-streaming data into an RDBMS instead of Redis
- Intercom feature? I really, really hope we don't need this (managing multiple
  input devices sounds like UX hell). But it may be better than using two
  applications for some people
